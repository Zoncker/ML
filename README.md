# Machine Learning course
Some ML labs and theory

-----
Nearest Neighbors
-----
Модуль [**sklearn.neighbors**](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)
предоставляет пользователю функционал *neighbors-based* методов обучения с учителем и без. 
Метод ближайших соседов **(NN)** без учителя - основа для других подходов, например,
[***многомерного обучение***](http://scikit-learn.org/stable/modules/manifold.html) 
и [***спектральной кластеризации***](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html). 
Методы с учителем приводят к классификации для данных с дискретными ярлыками и регрессии - для непрерывных.

#### Unsupervised Nearest Neighbors

Три алгоритма: 

#####Brute Force

Методы быстрых вычислений ближайших соседей активно исследуются до сих пор. 
Наиболее *"наивный"* поиск соседей включает в себя *"грубые"* расчёты расстояний между всеми парами точек в выборке:
для **N** образцов с размерностью **D** этот подход оценивается в ![](https://latex.codecogs.com/svg.latex?O%5BD%20N%5E2%5D). Рационально использовать данный подход
для малых объёмов данных, потому как по мере роста выборки, согласно оценке, стремительно возрастает и сложность, делая 
метод неприменимым.

#####K-Dimensional Tree

Для замещения неэффективности использования метода выше были созданы разнообразные структуры данных, основанные на ***деревьях***.
В общем случае, эти структуры пытаются уменьшить необходимое количество вычислений расстояний, эффективно кодируя 
информацию о совокупном расстоянии для выборки. Допустим, **A** очень далека от **B**, а **B** очень близка к **C**. 
Т.е, известно, что **A** очень далека от **C** без явного вычисления расстояния. (Транзитивность?). Как следствие,
стоимость вычилсений падает до  ![](https://latex.codecogs.com/svg.latex?O%5BD%20N%20%5Clog%28N%29%5D) или ниже. 
Разница с брутфорсом значительная (на **N** ).

Для реализации преимущества обобщённых вычислений были созданы **K - размерные деревья**, которые обобщают 
***двумерные квад-деревья*** и трёхмерные ***окт-деревья*** для произвольной размерности. 

KD - бинарное дерево, которое рекурсивно разбивает пространство параметров вдоль осей выборки, разделяя его на вложенные
неоднородные (ортотропные) области, которые заполнены объектами выборки. Такое дерево строится очень быстро, 
поскольку разбиение производится только вдоль осей выборки, вычисления D-мерных расстояний не требуются. Единожды
построенное, ближайший сосед любой точки может быть найден за ![](https://latex.codecogs.com/svg.latex?O%5B%5Clog%28N%29%5D)
Хоть данный подход и очень быстр для маломерных **(D<20)** случаев, он становится неэффективным с существеным ростом **D**,
 имеет место быть т.н. ***"проклятие размерности"***.
 
#####Ball-Tree

Данный метод решает проблему неэффективности KD - деревьев для многомерных случаев. Используется структура **ball tree**.
Вместо разбиения данных вдоль осей прямоугольной системы, проводится разбиение данных в последовательность вложенных гиперсфер.
Это делает операцию построения дерева несколько более дорогостоящей в сравнении с KD, но приводит к намного более 
удобной структуре, даже при работе с большими размерностями. 

Дерево-шар рекурсивно разбивает данные в ячейки, определённые центроидом **C** и радиусом **r**, такими, что каждая точка 
ячейки лежит внутри гиперсферы от **r** и **C**. Количество точек - кандидатов уменьшено использованием ***неравенства
треугольника***: ![](https://latex.codecogs.com/svg.latex?%7Cx&plus;y%7C%20%5Cleq%20%7Cx%7C%20&plus;%20%7Cy%7C)
Таким образом, одного расчёта расстояния между тестовой точкой и центроидом достаточно для определения нижней и верхней 
грани расстояний ко всем точкам ячейки. Благодаря сферической геометрии ячеек, метод превосходит KD в случае больших
размерностей, пусть и реальная производительность существенно зависит от структуры обучающей выборки.
