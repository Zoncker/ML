
Задачу обучения по прецедентам ![](https://latex.codecogs.com/svg.latex?Y%20%3D%20%5Cmathbb%7BR%7D) принято называть *восстановлением регрессии*. Постановка задачи аналогична. 

Модель алгоритмов задана в виде парам-кого семейства функций ![](https://latex.codecogs.com/svg.latex?%5Cinline%20f%28x%2C%5Calpha%20%29), а ![](https://latex.codecogs.com/svg.latex?%5Cinline%20%5Calpha%20%5Cin%20%5Cmathbb%7BR%7D%5Ep) - вектор параметров.


Функционал качества аппроксимации целевой зависимости на выборке - сумма квадратов ошибок - *остаточная сумма квадратов, RSS*: 

![](https://latex.codecogs.com/svg.latex?%5Cinline%20Q%28%5Calpha%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%3D1%7D%5Elw_i%28f%28x_i%2C%5Calpha%29-y_i%29%5E2)
, где ![](https://latex.codecogs.com/svg.latex?%5Cinline%20w_i) - вес, важность объекта i. 


Обучение по методу наименьших квадратов - поиск вектора параметров ![](https://latex.codecogs.com/svg.latex?%5Calpha^*), где достигается минимум среднего квадрата ошибки на выборке: 

![](https://latex.codecogs.com/svg.latex?%5Calpha%5E*%20%3D%20%5Carg%5Cmin_%7B%5Calpha%5Cin%5Cmathbb%7BR%7D%5Ep%7D%20Q%28%5Calpha%2C%20X%5El%29)

Решение оптимизационной задачи - использование необходимого условия минимума. Оно же и принимается за искомый вектор ![](https://latex.codecogs.com/svg.latex?%5Calpha^*).

### Непараметрическая регрессия. Ядерное сглаживание.

Непараметрическое восстановление регрессии основано на той же идее, непараметрическое восстановление плотности распределения.


Значение *a(x)* вычисляется для каждого объекта по нескольким ближайшим к нему объектам выборки. Для оценки близости на множестве должна быть задана функция расстояния ![](https://latex.codecogs.com/svg.latex?%5Crho%28x%2Cx%5E%7B%27%7D%29)
